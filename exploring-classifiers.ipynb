{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61455c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neuroqwerty as nq #neuroqwerty is a .py file created by me\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d8060",
   "metadata": {},
   "source": [
    "# Introduction to Parkinson's Disease and Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b29aff",
   "metadata": {},
   "source": [
    "Parkinson's disease(PD) is a neurodegenerative disorder related to movement. This means that as time continues, symptoms will worsen. Symptoms of people with PD include tremors, stiffness, difficulty speaking, loss of balance and difficulty initiating movement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b641ab5a",
   "metadata": {},
   "source": [
    "Typically, PD is diagnosed by a clinician who looks at symptoms, medical history and tests to reach a diagnosis. However, utilizing a classifier that relies on keyboard typing data allows for the early detection of PD without a clinician. This could revolutionize the way that diagnoses are made. The best course of action is to use the classifier as a first step for early detection and then seeking out a doctor to properly diagnose it. A clinician with expertise in PD will not only diagnose but also find the best treatments for the individual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6667ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "myClass = nq.neuroqwerty()\n",
    "cs1Pd = pd.read_csv('MIT-CSXPD_v2/MIT-CS1PD/GT_DataPD_MIT-CS1PD.csv')\n",
    "cs2Pd = pd.read_csv('MIT-CSXPD_v2/MIT-CS2PD/GT_DataPD_MIT-CS2PD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f3d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs1Pd = myClass.multiplecsv_to_df_two_files(cs1Pd, 'MIT-CSXPD_v2/MIT-CS1PD/data_MIT-CS1PD/', 'file_1', 'file_2') \n",
    "cs1Pd = cs1Pd.drop(columns=['file_2'])\n",
    "\n",
    "cs2Pd = myClass.multiplecsv_to_df_left_right(cs2Pd, 'MIT-CSXPD_v2/MIT-CS2PD/data_MIT-CS2PD/')\n",
    "\n",
    "combine = [cs1Pd, cs2Pd]\n",
    "total_df =  pd.concat(combine, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcf9ab",
   "metadata": {},
   "source": [
    "Most of the libraries can be used by you, the user, with the exception of *neuroqwerty* which is a .py file, created by me, that contains functions to parse through each of the 85 people's individual dataset, extract key information, and place that information into a singular dataset. \n",
    "Since there were two experiments done, two different datasets were created called *cs1Pd* and *cs2Pd*. These two datasets were then combined into one dataset called *total_df* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b0870",
   "metadata": {},
   "source": [
    "If you're curious about the dataset, it can be found here at: https://physionet.org/content/nqmitcsxpd/1.0.0/MIT-CS1PD/#files-panel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9241a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_df.drop(columns=['pID', 'gt', 'updrs108', 'nqScore', 'file_1', 'afTap' , 'sTap'])\n",
    "y = total_df['gt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9fa00",
   "metadata": {},
   "source": [
    "The data is then divided into X(features) and y(labels). X has 67 features. X contains information from the keyboard typing dataset only. Focusing on typing data allows the classifier to be made available to the public as long as there is a keyboard. This is why features like 'updrs108', 'afTap' and 'sTap' that evaluate PD in a clinical setting were removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39201a5",
   "metadata": {},
   "source": [
    "# Exploration of different classifiers with different techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ba6d9",
   "metadata": {},
   "source": [
    "Here we explore 11 different classifiers along with different techniques to see the wall time(how long a cell takes to run) and the performance metrics(measured using accuracy and AUC). This exploration will lead to finding the best classifier based on these metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e950354",
   "metadata": {},
   "source": [
    "Here we explore 11 different classifiers:\n",
    "1. GaussianNB: stands for Gaussian Naive Bayes\n",
    "2. SVC: stands for Support Vector Classification\n",
    "3. NuSVC: stands for Nu-support Vector Classification\n",
    "4. KNeigborsClassifier: stands for K-Nearest Neighbors\n",
    "5. MLPClassifier: stands for Multi-Layer Perceptron\n",
    "6. DecisionTreeClassifier \n",
    "7. RandomForestClassifier \n",
    "8. LogisticRegression \n",
    "9. AdaBoostClassifier \n",
    "10. GradientBoostingClassifier\n",
    "11. XGBClassifier: stands for Extreme Gradient Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f825433",
   "metadata": {},
   "source": [
    "Different techniques explored were: \n",
    "- Standardization: scaling so that features are have a mean of 0 and a standard deviation of 1\n",
    "- Feature selection: selecting the best 20 features using SelectKBest\n",
    "- Bagging (or booststrap aggregation): ensemble learning technique that trains multiple models on bootstrapped data independently, generates predictions for each individual model, and then aggregates the predictions by using a majority voting approach to get the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d656e8",
   "metadata": {},
   "source": [
    "### Baseline models: exploring models without any additional techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648c201",
   "metadata": {},
   "source": [
    "For each individual classifier(e.g. DecisionTree), the dataset was split into training and testing sets 150 different times. Each time, the accuracy and AUC was found. After 150 times, the average of the accuracy and AUC was calculated, as well as the median of the accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69bf60b",
   "metadata": {},
   "source": [
    "For the code below, I took inspiration from a Kaggle notebook on prediction of Alzheimer's. The link to the notebook is: https://www.kaggle.com/code/gallo33henrique/ml-alzheimer-predict/notebook  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d100b4",
   "metadata": {},
   "source": [
    "Side note: the code here is very similar to the code used in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad1e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 54.3 s, total: 3min 33s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = [GaussianNB(),\n",
    "          SVC(probability=True),\n",
    "          NuSVC(probability = True, nu = .30),\n",
    "          KNeighborsClassifier(),\n",
    "          MLPClassifier(), \n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(n_estimators=100), \n",
    "          LogisticRegression(),\n",
    "          AdaBoostClassifier(),\n",
    "          GradientBoostingClassifier(),\n",
    "          xgb.XGBClassifier()\n",
    "         ]\n",
    "\n",
    "# List to store metrics for each model\n",
    "metricas = []\n",
    "num = 150\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for i in range(num): \n",
    "        rand_ = random.randint(0, 10000) \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand_)\n",
    "        \n",
    "        # Create the classifier\n",
    "        classifier = model\n",
    "\n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_proba = classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #Calculate AUC \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "        \n",
    "        acc_list.append(acc_val)\n",
    "        auc_list.append(auc_val)\n",
    "\n",
    "    acc_avg = np.mean(acc_list)\n",
    "    acc_median = np.median(acc_list)\n",
    "    auc_avg = np.mean(auc_list)\n",
    "    \n",
    "    # Extract metrics of interest from the report\n",
    "    metrics = {\"Model\": type(model).__name__,\n",
    "               \"Average Accuracy\": acc_avg,\n",
    "               \"Median Accuracy\": acc_median,\n",
    "               \"AUC Average\": auc_avg\n",
    "              }\n",
    "    metricas.append(metrics)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metricas = pd.DataFrame(metricas)\n",
    "\n",
    "# Function to highlight the maximum value in each column\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the highlighting function\n",
    "df_metricas_styled = df_metricas.style.apply(highlight_max, subset=['Average Accuracy', 'Median Accuracy', \n",
    "                                                                   'AUC Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51719b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c79e0_row6_col2, #T_c79e0_row8_col1, #T_c79e0_row8_col2, #T_c79e0_row8_col3 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c79e0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c79e0_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_c79e0_level0_col1\" class=\"col_heading level0 col1\" >Average Accuracy</th>\n",
       "      <th id=\"T_c79e0_level0_col2\" class=\"col_heading level0 col2\" >Median Accuracy</th>\n",
       "      <th id=\"T_c79e0_level0_col3\" class=\"col_heading level0 col3\" >AUC Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c79e0_row0_col0\" class=\"data row0 col0\" >GaussianNB</td>\n",
       "      <td id=\"T_c79e0_row0_col1\" class=\"data row0 col1\" >0.651818</td>\n",
       "      <td id=\"T_c79e0_row0_col2\" class=\"data row0 col2\" >0.636364</td>\n",
       "      <td id=\"T_c79e0_row0_col3\" class=\"data row0 col3\" >0.687849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c79e0_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "      <td id=\"T_c79e0_row1_col1\" class=\"data row1 col1\" >0.412727</td>\n",
       "      <td id=\"T_c79e0_row1_col2\" class=\"data row1 col2\" >0.409091</td>\n",
       "      <td id=\"T_c79e0_row1_col3\" class=\"data row1 col3\" >0.547627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c79e0_row2_col0\" class=\"data row2 col0\" >NuSVC</td>\n",
       "      <td id=\"T_c79e0_row2_col1\" class=\"data row2 col1\" >0.458485</td>\n",
       "      <td id=\"T_c79e0_row2_col2\" class=\"data row2 col2\" >0.454545</td>\n",
       "      <td id=\"T_c79e0_row2_col3\" class=\"data row2 col3\" >0.473096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c79e0_row3_col0\" class=\"data row3 col0\" >KNeighborsClassifier</td>\n",
       "      <td id=\"T_c79e0_row3_col1\" class=\"data row3 col1\" >0.425455</td>\n",
       "      <td id=\"T_c79e0_row3_col2\" class=\"data row3 col2\" >0.409091</td>\n",
       "      <td id=\"T_c79e0_row3_col3\" class=\"data row3 col3\" >0.420915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c79e0_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_c79e0_row4_col1\" class=\"data row4 col1\" >0.530909</td>\n",
       "      <td id=\"T_c79e0_row4_col2\" class=\"data row4 col2\" >0.545455</td>\n",
       "      <td id=\"T_c79e0_row4_col3\" class=\"data row4 col3\" >0.550304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c79e0_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_c79e0_row5_col1\" class=\"data row5 col1\" >0.606667</td>\n",
       "      <td id=\"T_c79e0_row5_col2\" class=\"data row5 col2\" >0.590909</td>\n",
       "      <td id=\"T_c79e0_row5_col3\" class=\"data row5 col3\" >0.611620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c79e0_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_c79e0_row6_col1\" class=\"data row6 col1\" >0.666061</td>\n",
       "      <td id=\"T_c79e0_row6_col2\" class=\"data row6 col2\" >0.681818</td>\n",
       "      <td id=\"T_c79e0_row6_col3\" class=\"data row6 col3\" >0.738963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c79e0_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_c79e0_row7_col1\" class=\"data row7 col1\" >0.552121</td>\n",
       "      <td id=\"T_c79e0_row7_col2\" class=\"data row7 col2\" >0.545455</td>\n",
       "      <td id=\"T_c79e0_row7_col3\" class=\"data row7 col3\" >0.586090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c79e0_row8_col0\" class=\"data row8 col0\" >AdaBoostClassifier</td>\n",
       "      <td id=\"T_c79e0_row8_col1\" class=\"data row8 col1\" >0.675455</td>\n",
       "      <td id=\"T_c79e0_row8_col2\" class=\"data row8 col2\" >0.681818</td>\n",
       "      <td id=\"T_c79e0_row8_col3\" class=\"data row8 col3\" >0.741397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c79e0_row9_col0\" class=\"data row9 col0\" >GradientBoostingClassifier</td>\n",
       "      <td id=\"T_c79e0_row9_col1\" class=\"data row9 col1\" >0.646970</td>\n",
       "      <td id=\"T_c79e0_row9_col2\" class=\"data row9 col2\" >0.636364</td>\n",
       "      <td id=\"T_c79e0_row9_col3\" class=\"data row9 col3\" >0.733407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c79e0_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_c79e0_row10_col0\" class=\"data row10 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_c79e0_row10_col1\" class=\"data row10 col1\" >0.639697</td>\n",
       "      <td id=\"T_c79e0_row10_col2\" class=\"data row10 col2\" >0.636364</td>\n",
       "      <td id=\"T_c79e0_row10_col3\" class=\"data row10 col3\" >0.700430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc1828654f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metricas_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc0b46",
   "metadata": {},
   "source": [
    "It takes 1 minute with 3 seconds to run the baseline models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7a331",
   "metadata": {},
   "source": [
    "The best classifier is AdaBoost with an average accuracy of .675 and an average AUC of .741"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332182a",
   "metadata": {},
   "source": [
    "### Standardization: exploring models with standardization only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba69ce8",
   "metadata": {},
   "source": [
    "Standardization is applied to each feature where each feature is scaled to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2a6e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 57.5 s, total: 3min 55s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          SVC(probability=True),\n",
    "          NuSVC(probability = True, nu = .30),\n",
    "          KNeighborsClassifier(),\n",
    "          MLPClassifier(), \n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(n_estimators=100), \n",
    "          LogisticRegression(),\n",
    "          AdaBoostClassifier(), \n",
    "          GradientBoostingClassifier(),\n",
    "          xgb.XGBClassifier()\n",
    "         ]\n",
    "\n",
    "# List to store metrics for each model\n",
    "metricas_standardized = []\n",
    "num = 150\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for i in range(num): \n",
    "        rand_ = random.randint(0, 10000) \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand_)\n",
    "        \n",
    "        #Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "        \n",
    "        # Create the classifier\n",
    "        classifier = model\n",
    "\n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_proba = classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #Calculate AUC \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "                \n",
    "        acc_list.append(acc_val)\n",
    "        auc_list.append(auc_val)\n",
    "        \n",
    "    acc_avg = np.mean(acc_list)\n",
    "    acc_median = np.median(acc_list)\n",
    "    auc_avg = np.mean(auc_list)\n",
    "    # Extract metrics of interest from the report\n",
    "    metrics_standardized = {\"Model\": type(model).__name__,\n",
    "               \"Average Accuracy\": acc_avg,\n",
    "               \"Median Accuracy\": acc_median,\n",
    "               \"AUC Average\": auc_avg\n",
    "              }\n",
    "    metricas_standardized.append(metrics_standardized)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metricas_standard = pd.DataFrame(metricas_standardized)\n",
    "\n",
    "# Function to highlight the maximum value in each column\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the highlighting function\n",
    "df_metricas_styled_standardized = df_metricas_standard.style.apply(highlight_max, subset=['Average Accuracy', 'Median Accuracy', \n",
    "                                                                   'AUC Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfcd34c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_84188_row6_col1, #T_84188_row6_col2, #T_84188_row6_col3 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_84188\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_84188_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_84188_level0_col1\" class=\"col_heading level0 col1\" >Average Accuracy</th>\n",
       "      <th id=\"T_84188_level0_col2\" class=\"col_heading level0 col2\" >Median Accuracy</th>\n",
       "      <th id=\"T_84188_level0_col3\" class=\"col_heading level0 col3\" >AUC Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_84188_row0_col0\" class=\"data row0 col0\" >GaussianNB</td>\n",
       "      <td id=\"T_84188_row0_col1\" class=\"data row0 col1\" >0.645758</td>\n",
       "      <td id=\"T_84188_row0_col2\" class=\"data row0 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row0_col3\" class=\"data row0 col3\" >0.649816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_84188_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "      <td id=\"T_84188_row1_col1\" class=\"data row1 col1\" >0.625455</td>\n",
       "      <td id=\"T_84188_row1_col2\" class=\"data row1 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row1_col3\" class=\"data row1 col3\" >0.648241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_84188_row2_col0\" class=\"data row2 col0\" >NuSVC</td>\n",
       "      <td id=\"T_84188_row2_col1\" class=\"data row2 col1\" >0.622424</td>\n",
       "      <td id=\"T_84188_row2_col2\" class=\"data row2 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row2_col3\" class=\"data row2 col3\" >0.637466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_84188_row3_col0\" class=\"data row3 col0\" >KNeighborsClassifier</td>\n",
       "      <td id=\"T_84188_row3_col1\" class=\"data row3 col1\" >0.580000</td>\n",
       "      <td id=\"T_84188_row3_col2\" class=\"data row3 col2\" >0.590909</td>\n",
       "      <td id=\"T_84188_row3_col3\" class=\"data row3 col3\" >0.628848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_84188_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_84188_row4_col1\" class=\"data row4 col1\" >0.630000</td>\n",
       "      <td id=\"T_84188_row4_col2\" class=\"data row4 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row4_col3\" class=\"data row4 col3\" >0.690782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_84188_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_84188_row5_col1\" class=\"data row5 col1\" >0.601212</td>\n",
       "      <td id=\"T_84188_row5_col2\" class=\"data row5 col2\" >0.590909</td>\n",
       "      <td id=\"T_84188_row5_col3\" class=\"data row5 col3\" >0.607433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_84188_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_84188_row6_col1\" class=\"data row6 col1\" >0.676667</td>\n",
       "      <td id=\"T_84188_row6_col2\" class=\"data row6 col2\" >0.681818</td>\n",
       "      <td id=\"T_84188_row6_col3\" class=\"data row6 col3\" >0.742290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_84188_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_84188_row7_col1\" class=\"data row7 col1\" >0.654545</td>\n",
       "      <td id=\"T_84188_row7_col2\" class=\"data row7 col2\" >0.659091</td>\n",
       "      <td id=\"T_84188_row7_col3\" class=\"data row7 col3\" >0.727960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_84188_row8_col0\" class=\"data row8 col0\" >AdaBoostClassifier</td>\n",
       "      <td id=\"T_84188_row8_col1\" class=\"data row8 col1\" >0.633333</td>\n",
       "      <td id=\"T_84188_row8_col2\" class=\"data row8 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row8_col3\" class=\"data row8 col3\" >0.691412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_84188_row9_col0\" class=\"data row9 col0\" >GradientBoostingClassifier</td>\n",
       "      <td id=\"T_84188_row9_col1\" class=\"data row9 col1\" >0.650303</td>\n",
       "      <td id=\"T_84188_row9_col2\" class=\"data row9 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row9_col3\" class=\"data row9 col3\" >0.722210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84188_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_84188_row10_col0\" class=\"data row10 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_84188_row10_col1\" class=\"data row10 col1\" >0.646970</td>\n",
       "      <td id=\"T_84188_row10_col2\" class=\"data row10 col2\" >0.636364</td>\n",
       "      <td id=\"T_84188_row10_col3\" class=\"data row10 col3\" >0.705146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc190463280>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metricas_styled_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94899a39",
   "metadata": {},
   "source": [
    "It takes 1 minute with 6 seconds to run this code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b003d2",
   "metadata": {},
   "source": [
    "The best classifier based on accuracy is random forest with an average accuracy of .677 and an average AUC of .742"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a985f",
   "metadata": {},
   "source": [
    "When comparing the standardized models to the baseline models, the models that improved significantly were SVC, NuSVC, KNeighbors, MLP and LogisticRegression. Standardization is seen as a good preprocessing step before training a model, which is definitely true for k-nearest neighbors, SVM's, and neural networks. Although it's good to standardize data, there was a classifier that performed significantly worse. That classifier was AdaBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b68a6e",
   "metadata": {},
   "source": [
    "### Feature Selection: exploring models with the 20 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f0ebb",
   "metadata": {},
   "source": [
    "Each classifier uses SelectKBest to find the 20 best features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f38b947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 s, sys: 2 s, total: 59.8 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          SVC(probability=True),\n",
    "          NuSVC(probability = True, nu = .30),\n",
    "          KNeighborsClassifier(),\n",
    "          MLPClassifier(), \n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(n_estimators=100), \n",
    "          LogisticRegression(),\n",
    "          AdaBoostClassifier(),\n",
    "          GradientBoostingClassifier(),\n",
    "          xgb.XGBClassifier()\n",
    "         ]\n",
    "\n",
    "# List to store metrics for each model\n",
    "metricas_feat = []\n",
    "num = 150\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for i in range(num): \n",
    "        rand_ = random.randint(0, 10000) \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand_)\n",
    "        \n",
    "        # Select the 20 best features for the training and testing set\n",
    "        selector = SelectKBest(f_classif, k=20)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "        \n",
    "        # Create the classifier\n",
    "        classifier = model\n",
    "\n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        y_proba = classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #Calculate AUC \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "                \n",
    "        acc_list.append(acc_val)\n",
    "        auc_list.append(auc_val)\n",
    "\n",
    "    acc_avg = np.mean(acc_list)\n",
    "    acc_median = np.median(acc_list)\n",
    "    auc_avg = np.mean(auc_list)\n",
    "    # Extract metrics of interest from the report\n",
    "    metrics_feat = {\"Model\": type(model).__name__,\n",
    "               \"Average Accuracy\": acc_avg,\n",
    "               \"Median Accuracy\": acc_median,\n",
    "               \"AUC Average\": auc_avg\n",
    "              }\n",
    "    metricas_feat.append(metrics_feat)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metricas_feat = pd.DataFrame(metricas_feat)\n",
    "\n",
    "# Function to highlight the maximum value in each column\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the highlighting function\n",
    "df_metricas_styled_feat = df_metricas_feat.style.apply(highlight_max, subset=['Average Accuracy', 'Median Accuracy', \n",
    "                                                                   'AUC Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28318cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_19f70_row6_col3, #T_19f70_row8_col1, #T_19f70_row8_col2 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_19f70\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_19f70_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_19f70_level0_col1\" class=\"col_heading level0 col1\" >Average Accuracy</th>\n",
       "      <th id=\"T_19f70_level0_col2\" class=\"col_heading level0 col2\" >Median Accuracy</th>\n",
       "      <th id=\"T_19f70_level0_col3\" class=\"col_heading level0 col3\" >AUC Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_19f70_row0_col0\" class=\"data row0 col0\" >GaussianNB</td>\n",
       "      <td id=\"T_19f70_row0_col1\" class=\"data row0 col1\" >0.664545</td>\n",
       "      <td id=\"T_19f70_row0_col2\" class=\"data row0 col2\" >0.636364</td>\n",
       "      <td id=\"T_19f70_row0_col3\" class=\"data row0 col3\" >0.690468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_19f70_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "      <td id=\"T_19f70_row1_col1\" class=\"data row1 col1\" >0.444848</td>\n",
       "      <td id=\"T_19f70_row1_col2\" class=\"data row1 col2\" >0.431818</td>\n",
       "      <td id=\"T_19f70_row1_col3\" class=\"data row1 col3\" >0.480101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_19f70_row2_col0\" class=\"data row2 col0\" >NuSVC</td>\n",
       "      <td id=\"T_19f70_row2_col1\" class=\"data row2 col1\" >0.571818</td>\n",
       "      <td id=\"T_19f70_row2_col2\" class=\"data row2 col2\" >0.590909</td>\n",
       "      <td id=\"T_19f70_row2_col3\" class=\"data row2 col3\" >0.600120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_19f70_row3_col0\" class=\"data row3 col0\" >KNeighborsClassifier</td>\n",
       "      <td id=\"T_19f70_row3_col1\" class=\"data row3 col1\" >0.478182</td>\n",
       "      <td id=\"T_19f70_row3_col2\" class=\"data row3 col2\" >0.500000</td>\n",
       "      <td id=\"T_19f70_row3_col3\" class=\"data row3 col3\" >0.496843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_19f70_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_19f70_row4_col1\" class=\"data row4 col1\" >0.597576</td>\n",
       "      <td id=\"T_19f70_row4_col2\" class=\"data row4 col2\" >0.590909</td>\n",
       "      <td id=\"T_19f70_row4_col3\" class=\"data row4 col3\" >0.620693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_19f70_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_19f70_row5_col1\" class=\"data row5 col1\" >0.630606</td>\n",
       "      <td id=\"T_19f70_row5_col2\" class=\"data row5 col2\" >0.636364</td>\n",
       "      <td id=\"T_19f70_row5_col3\" class=\"data row5 col3\" >0.634653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_19f70_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_19f70_row6_col1\" class=\"data row6 col1\" >0.696667</td>\n",
       "      <td id=\"T_19f70_row6_col2\" class=\"data row6 col2\" >0.681818</td>\n",
       "      <td id=\"T_19f70_row6_col3\" class=\"data row6 col3\" >0.772682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_19f70_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_19f70_row7_col1\" class=\"data row7 col1\" >0.599697</td>\n",
       "      <td id=\"T_19f70_row7_col2\" class=\"data row7 col2\" >0.590909</td>\n",
       "      <td id=\"T_19f70_row7_col3\" class=\"data row7 col3\" >0.653705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_19f70_row8_col0\" class=\"data row8 col0\" >AdaBoostClassifier</td>\n",
       "      <td id=\"T_19f70_row8_col1\" class=\"data row8 col1\" >0.697879</td>\n",
       "      <td id=\"T_19f70_row8_col2\" class=\"data row8 col2\" >0.727273</td>\n",
       "      <td id=\"T_19f70_row8_col3\" class=\"data row8 col3\" >0.761220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_19f70_row9_col0\" class=\"data row9 col0\" >GradientBoostingClassifier</td>\n",
       "      <td id=\"T_19f70_row9_col1\" class=\"data row9 col1\" >0.678182</td>\n",
       "      <td id=\"T_19f70_row9_col2\" class=\"data row9 col2\" >0.681818</td>\n",
       "      <td id=\"T_19f70_row9_col3\" class=\"data row9 col3\" >0.753826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_19f70_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_19f70_row10_col0\" class=\"data row10 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_19f70_row10_col1\" class=\"data row10 col1\" >0.659394</td>\n",
       "      <td id=\"T_19f70_row10_col2\" class=\"data row10 col2\" >0.681818</td>\n",
       "      <td id=\"T_19f70_row10_col3\" class=\"data row10 col3\" >0.735868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc190445460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metricas_styled_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf60ec3",
   "metadata": {},
   "source": [
    "It takes 39.3 seconds to run this code. This is slightly faster than the previous two explorations because the previous explorations relied on all 67 features, whereas, this one relies on 20 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b14e9",
   "metadata": {},
   "source": [
    "In terms of accuracy, the best classifier is AdaBoost with an average accuracy of .698. In terms of AUC, the best classifier is random forest with an an AUC of .773."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d5fcf",
   "metadata": {},
   "source": [
    "When comparing the 20-best features models to the baseline models, most models improved. The only decrease was for the support vector classifier in which the average AUC decreased from 0.548 to .480, but the average accuracy increased from .413 to .445. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e78e20",
   "metadata": {},
   "source": [
    "### Exploring classifiers with standardization,  feature selection and bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf1b2ca",
   "metadata": {},
   "source": [
    "The next two explorations will look at bagging which is an excessively timely procedure. Because of this, instead of looking at each classifier 150 times like we previously did, we will look at each classifier 75 times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283fb49",
   "metadata": {},
   "source": [
    "For this section, each classifier uses all three techniques: standardization, feature selection and bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a560a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 44s, sys: 1min, total: 30min 45s\n",
      "Wall time: 21min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          SVC(probability=True),\n",
    "          NuSVC(probability = True, nu = .30),\n",
    "          KNeighborsClassifier(),\n",
    "          MLPClassifier(), \n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(n_estimators=100), \n",
    "          LogisticRegression(),\n",
    "          AdaBoostClassifier(), \n",
    "          GradientBoostingClassifier(),\n",
    "          xgb.XGBClassifier()\n",
    "         ]\n",
    "\n",
    "# List to store metrics for each model\n",
    "metricas_stand_feat_bag = []\n",
    "num = 75\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for i in range(num): \n",
    "        rand_ = random.randint(0, 10000) \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand_)\n",
    "        \n",
    "        #Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "        X_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n",
    "        \n",
    "        # Select the 20 best features for the training and testing set\n",
    "        selector = SelectKBest(f_classif, k=20)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "        \n",
    "        # Create the base classifier\n",
    "        base_classifier = model\n",
    "\n",
    "        # Number of base models (iterations)\n",
    "        n_estimators = 75\n",
    "        \n",
    "        # Create the Bagging classifier\n",
    "        bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=n_estimators)\n",
    "\n",
    "        # Train the Bagging classifier\n",
    "        bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = bagging_classifier.predict(X_test)\n",
    "        y_proba = bagging_classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #Calculate AUC \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "                \n",
    "        acc_list.append(acc_val)\n",
    "        auc_list.append(auc_val)\n",
    "\n",
    "    acc_avg = np.mean(acc_list)\n",
    "    acc_median = np.median(acc_list)\n",
    "    auc_avg = np.mean(auc_list)\n",
    "    \n",
    "    # Extract metrics of interest from the report\n",
    "    metrics_stand_feat_bag = {\"Model\": type(model).__name__,\n",
    "               \"Average Accuracy\": acc_avg,\n",
    "               \"Median Accuracy\": acc_median,\n",
    "               \"AUC Average\": auc_avg\n",
    "              }\n",
    "    metricas_stand_feat_bag.append(metrics_stand_feat_bag)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metricas_stand_feat_bag = pd.DataFrame(metricas_stand_feat_bag)\n",
    "\n",
    "# Function to highlight the maximum value in each column\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the highlighting function\n",
    "df_metricas_styled_stand_feat_bag = df_metricas_stand_feat_bag.style.apply(highlight_max, subset=['Average Accuracy', 'Median Accuracy', \n",
    "                                                                   'AUC Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b02585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_48fec_row0_col2, #T_48fec_row4_col2, #T_48fec_row5_col2, #T_48fec_row6_col2, #T_48fec_row7_col2, #T_48fec_row8_col1, #T_48fec_row8_col2, #T_48fec_row8_col3, #T_48fec_row9_col2 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_48fec\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_48fec_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_48fec_level0_col1\" class=\"col_heading level0 col1\" >Average Accuracy</th>\n",
       "      <th id=\"T_48fec_level0_col2\" class=\"col_heading level0 col2\" >Median Accuracy</th>\n",
       "      <th id=\"T_48fec_level0_col3\" class=\"col_heading level0 col3\" >AUC Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_48fec_row0_col0\" class=\"data row0 col0\" >GaussianNB</td>\n",
       "      <td id=\"T_48fec_row0_col1\" class=\"data row0 col1\" >0.646667</td>\n",
       "      <td id=\"T_48fec_row0_col2\" class=\"data row0 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row0_col3\" class=\"data row0 col3\" >0.689175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_48fec_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "      <td id=\"T_48fec_row1_col1\" class=\"data row1 col1\" >0.613333</td>\n",
       "      <td id=\"T_48fec_row1_col2\" class=\"data row1 col2\" >0.636364</td>\n",
       "      <td id=\"T_48fec_row1_col3\" class=\"data row1 col3\" >0.698191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_48fec_row2_col0\" class=\"data row2 col0\" >NuSVC</td>\n",
       "      <td id=\"T_48fec_row2_col1\" class=\"data row2 col1\" >0.644242</td>\n",
       "      <td id=\"T_48fec_row2_col2\" class=\"data row2 col2\" >0.636364</td>\n",
       "      <td id=\"T_48fec_row2_col3\" class=\"data row2 col3\" >0.746021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_48fec_row3_col0\" class=\"data row3 col0\" >KNeighborsClassifier</td>\n",
       "      <td id=\"T_48fec_row3_col1\" class=\"data row3 col1\" >0.616364</td>\n",
       "      <td id=\"T_48fec_row3_col2\" class=\"data row3 col2\" >0.590909</td>\n",
       "      <td id=\"T_48fec_row3_col3\" class=\"data row3 col3\" >0.704560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_48fec_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_48fec_row4_col1\" class=\"data row4 col1\" >0.677576</td>\n",
       "      <td id=\"T_48fec_row4_col2\" class=\"data row4 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row4_col3\" class=\"data row4 col3\" >0.758131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_48fec_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_48fec_row5_col1\" class=\"data row5 col1\" >0.689091</td>\n",
       "      <td id=\"T_48fec_row5_col2\" class=\"data row5 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row5_col3\" class=\"data row5 col3\" >0.741349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_48fec_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_48fec_row6_col1\" class=\"data row6 col1\" >0.679394</td>\n",
       "      <td id=\"T_48fec_row6_col2\" class=\"data row6 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row6_col3\" class=\"data row6 col3\" >0.763058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_48fec_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_48fec_row7_col1\" class=\"data row7 col1\" >0.672727</td>\n",
       "      <td id=\"T_48fec_row7_col2\" class=\"data row7 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row7_col3\" class=\"data row7 col3\" >0.747257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_48fec_row8_col0\" class=\"data row8 col0\" >AdaBoostClassifier</td>\n",
       "      <td id=\"T_48fec_row8_col1\" class=\"data row8 col1\" >0.700606</td>\n",
       "      <td id=\"T_48fec_row8_col2\" class=\"data row8 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row8_col3\" class=\"data row8 col3\" >0.776442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_48fec_row9_col0\" class=\"data row9 col0\" >GradientBoostingClassifier</td>\n",
       "      <td id=\"T_48fec_row9_col1\" class=\"data row9 col1\" >0.658788</td>\n",
       "      <td id=\"T_48fec_row9_col2\" class=\"data row9 col2\" >0.681818</td>\n",
       "      <td id=\"T_48fec_row9_col3\" class=\"data row9 col3\" >0.744806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_48fec_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_48fec_row10_col0\" class=\"data row10 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_48fec_row10_col1\" class=\"data row10 col1\" >0.650909</td>\n",
       "      <td id=\"T_48fec_row10_col2\" class=\"data row10 col2\" >0.636364</td>\n",
       "      <td id=\"T_48fec_row10_col3\" class=\"data row10 col3\" >0.733117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc1a0bdf5b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metricas_styled_stand_feat_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f35ddd",
   "metadata": {},
   "source": [
    "It takes 21 minutes with 19 seconds to run this code. Bagging is a computationaly costly procedure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31b392",
   "metadata": {},
   "source": [
    "The best classifier based on accuracy is AdaBoost with an average accuracy of .701 and an average AUC of .776"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ab8fc",
   "metadata": {},
   "source": [
    "When comparing the average AUC of the baseline classifier with those of the classifiers with standardization, feature selection and bagging, most of the models improved. The AUC for gaussian naive bayes stayed roughly the same. When comparing the average accuracy, most of the classifiers increased. The naive bayes decreased slightly from .652 to .647."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384fbf2",
   "metadata": {},
   "source": [
    "### Exploring classifiers with feature selection and bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b4fcf",
   "metadata": {},
   "source": [
    "Each classifier uses two techniques: feature selection and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8147732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 26s, sys: 4min 39s, total: 38min 5s\n",
      "Wall time: 22min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = [GaussianNB(),\n",
    "          SVC(probability=True),\n",
    "          NuSVC(probability = True, nu = .30),\n",
    "          KNeighborsClassifier(),\n",
    "          MLPClassifier(), \n",
    "          DecisionTreeClassifier(),\n",
    "          RandomForestClassifier(n_estimators=100), \n",
    "          LogisticRegression(),\n",
    "          AdaBoostClassifier(),\n",
    "          GradientBoostingClassifier(),\n",
    "          xgb.XGBClassifier()\n",
    "         ]\n",
    "\n",
    "# List to store metrics for each model\n",
    "metricas_feat_bag = []\n",
    "num = 75\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    acc_list = []\n",
    "    auc_list = []\n",
    "    for i in range(num): \n",
    "        rand_ = random.randint(0, 10000) \n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand_)\n",
    "        \n",
    "        # Select the 20 best features for the training and testing set\n",
    "        selector = SelectKBest(f_classif, k=20)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "        \n",
    "        # Create the base classifier\n",
    "        base_classifier = model\n",
    "\n",
    "        # Number of base models (iterations)\n",
    "        n_estimators = 75\n",
    "        \n",
    "        # Create the Bagging classifier\n",
    "        bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=n_estimators)\n",
    "\n",
    "        # Train the Bagging classifier\n",
    "        bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = bagging_classifier.predict(X_test)\n",
    "        y_proba = bagging_classifier.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_val = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        #Calculate AUC \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "                \n",
    "        acc_list.append(acc_val)\n",
    "        auc_list.append(auc_val)\n",
    "\n",
    "    acc_avg = np.mean(acc_list)\n",
    "    acc_median = np.median(acc_list)\n",
    "    auc_avg = np.mean(auc_list)\n",
    "    \n",
    "    # Extract metrics of interest from the report\n",
    "    metrics_feat_bag = {\"Model\": type(model).__name__,\n",
    "               \"Average Accuracy\": acc_avg,\n",
    "               \"Median Accuracy\": acc_median,\n",
    "               \"AUC Average\": auc_avg\n",
    "              }\n",
    "    metricas_feat_bag.append(metrics_feat_bag)\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df_metricas_feat_bag = pd.DataFrame(metricas_feat_bag)\n",
    "\n",
    "# Function to highlight the maximum value in each column\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "# Apply the highlighting function\n",
    "df_metricas_styled_feat_bag = df_metricas_feat_bag.style.apply(highlight_max, subset=['Average Accuracy', 'Median Accuracy', \n",
    "                                                                   'AUC Average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6bf296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1c65d_row8_col1, #T_1c65d_row8_col2, #T_1c65d_row8_col3 {\n",
       "  background-color: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1c65d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1c65d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_1c65d_level0_col1\" class=\"col_heading level0 col1\" >Average Accuracy</th>\n",
       "      <th id=\"T_1c65d_level0_col2\" class=\"col_heading level0 col2\" >Median Accuracy</th>\n",
       "      <th id=\"T_1c65d_level0_col3\" class=\"col_heading level0 col3\" >AUC Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1c65d_row0_col0\" class=\"data row0 col0\" >GaussianNB</td>\n",
       "      <td id=\"T_1c65d_row0_col1\" class=\"data row0 col1\" >0.667879</td>\n",
       "      <td id=\"T_1c65d_row0_col2\" class=\"data row0 col2\" >0.681818</td>\n",
       "      <td id=\"T_1c65d_row0_col3\" class=\"data row0 col3\" >0.680312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_1c65d_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "      <td id=\"T_1c65d_row1_col1\" class=\"data row1 col1\" >0.433333</td>\n",
       "      <td id=\"T_1c65d_row1_col2\" class=\"data row1 col2\" >0.409091</td>\n",
       "      <td id=\"T_1c65d_row1_col3\" class=\"data row1 col3\" >0.463362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_1c65d_row2_col0\" class=\"data row2 col0\" >NuSVC</td>\n",
       "      <td id=\"T_1c65d_row2_col1\" class=\"data row2 col1\" >0.539394</td>\n",
       "      <td id=\"T_1c65d_row2_col2\" class=\"data row2 col2\" >0.545455</td>\n",
       "      <td id=\"T_1c65d_row2_col3\" class=\"data row2 col3\" >0.609905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_1c65d_row3_col0\" class=\"data row3 col0\" >KNeighborsClassifier</td>\n",
       "      <td id=\"T_1c65d_row3_col1\" class=\"data row3 col1\" >0.466061</td>\n",
       "      <td id=\"T_1c65d_row3_col2\" class=\"data row3 col2\" >0.454545</td>\n",
       "      <td id=\"T_1c65d_row3_col3\" class=\"data row3 col3\" >0.472609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_1c65d_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
       "      <td id=\"T_1c65d_row4_col1\" class=\"data row4 col1\" >0.599394</td>\n",
       "      <td id=\"T_1c65d_row4_col2\" class=\"data row4 col2\" >0.590909</td>\n",
       "      <td id=\"T_1c65d_row4_col3\" class=\"data row4 col3\" >0.656381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_1c65d_row5_col0\" class=\"data row5 col0\" >DecisionTreeClassifier</td>\n",
       "      <td id=\"T_1c65d_row5_col1\" class=\"data row5 col1\" >0.690909</td>\n",
       "      <td id=\"T_1c65d_row5_col2\" class=\"data row5 col2\" >0.681818</td>\n",
       "      <td id=\"T_1c65d_row5_col3\" class=\"data row5 col3\" >0.748693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_1c65d_row6_col0\" class=\"data row6 col0\" >RandomForestClassifier</td>\n",
       "      <td id=\"T_1c65d_row6_col1\" class=\"data row6 col1\" >0.689091</td>\n",
       "      <td id=\"T_1c65d_row6_col2\" class=\"data row6 col2\" >0.681818</td>\n",
       "      <td id=\"T_1c65d_row6_col3\" class=\"data row6 col3\" >0.748436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_1c65d_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_1c65d_row7_col1\" class=\"data row7 col1\" >0.589091</td>\n",
       "      <td id=\"T_1c65d_row7_col2\" class=\"data row7 col2\" >0.590909</td>\n",
       "      <td id=\"T_1c65d_row7_col3\" class=\"data row7 col3\" >0.644987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_1c65d_row8_col0\" class=\"data row8 col0\" >AdaBoostClassifier</td>\n",
       "      <td id=\"T_1c65d_row8_col1\" class=\"data row8 col1\" >0.723030</td>\n",
       "      <td id=\"T_1c65d_row8_col2\" class=\"data row8 col2\" >0.727273</td>\n",
       "      <td id=\"T_1c65d_row8_col3\" class=\"data row8 col3\" >0.798983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_1c65d_row9_col0\" class=\"data row9 col0\" >GradientBoostingClassifier</td>\n",
       "      <td id=\"T_1c65d_row9_col1\" class=\"data row9 col1\" >0.688485</td>\n",
       "      <td id=\"T_1c65d_row9_col2\" class=\"data row9 col2\" >0.681818</td>\n",
       "      <td id=\"T_1c65d_row9_col3\" class=\"data row9 col3\" >0.764994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1c65d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_1c65d_row10_col0\" class=\"data row10 col0\" >XGBClassifier</td>\n",
       "      <td id=\"T_1c65d_row10_col1\" class=\"data row10 col1\" >0.673939</td>\n",
       "      <td id=\"T_1c65d_row10_col2\" class=\"data row10 col2\" >0.681818</td>\n",
       "      <td id=\"T_1c65d_row10_col3\" class=\"data row10 col3\" >0.753238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc170927880>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metricas_styled_feat_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d398c",
   "metadata": {},
   "source": [
    "It took 22 minutes with 17 seconds to run this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf0d4d",
   "metadata": {},
   "source": [
    "The best classifier is AdaBoost with an average accuracy of .723 and an average AUC of .799"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a4ad3",
   "metadata": {},
   "source": [
    "When comparing the average AUC of the baseline classifier with those of the classifiers with feature selection and bagging, most models improved. The support vector's average AUC decreased from .548 to .463, and the naive bayes classifier stayed roughly the same. When comparing the average accuracy, all of the classifiers increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ef1f9",
   "metadata": {},
   "source": [
    "### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1db2c1",
   "metadata": {},
   "source": [
    "- Standardization: \n",
    " - allows features to be scaled so that they have similar ranges in values \n",
    " - improves accuracy when using logistic regression, support vector machines, neural networks and k-nearest neighbors classifiers\n",
    "- Feature selection:\n",
    " - cuts down on time to run because there is less features\n",
    " - improves accuracy by choosing the most important features for the model while discarding the least important ones\n",
    "- Bagging(or bootstrap aggregation): \n",
    " - computationally expensive procedure \n",
    " - improves accuracy by aggregating multiple individual models where each model has bootstrapped data\n",
    " - although costly, the best classifier uses bagging \n",
    "- Best classifier for this dataset:\n",
    " - was AdaBoost with both feature selection and bagging \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
